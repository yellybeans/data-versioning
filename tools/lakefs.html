<!DOCTYPE html>
<html lang="en">
<head>
    <title>Data Versioning</title>
    <meta charset="utf-8">
    <meta name="description" content="Data Versioning is rising because of the necessity of having a history for data analytics in a ML environment." />
    <meta name="keywords" content="data versioning, data version control, git-like data versioning, data analytics, model versioning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@innoq" />
    <meta name="twitter:title" content="Data Versioning" />
    <meta name="twitter:description" content="Data Versioning From a Historical Perspective" />
    <meta name="twitter:image" content="https://www.tbd.com/images/....png" />
    <meta name="twitter:image:alt" content="tbd" />
    <meta property="og:url" content="https://data-versioning.com" />
    <meta property="og:title" content="Data Versioning" />
    <meta property="og:description" content="Data Versioning From a Historical Perspective" />
    <meta property="og:image" content="https://www.tbd.com/images/....png" />

    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Book.woff2?cachebuster=2" crossorigin="">
    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Bold.woff2?cachebuster=2" crossorigin="">
    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Heavy.woff2?cachebuster=2" crossorigin="">
    <link rel="stylesheet" href="css/style.css" />
    <link rel="stylesheet" href="css/0.9.3_css_bulma.css" />
    <link rel="stylesheet" href="css/font-awesome_6.0.0_css_all.css"/>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
</head>
<body>
<section class="hero is-fullheight" style="background-color: #242424">

    <div class="hero-head">
        <nav class="navbar is-dark" role="navigation" aria-label="dropdown navigation">
            <div class="container">
                <div class="navbar-brand">
                    <span class="navbar-burger" data-target="navbarMenuHeroA">
            <span></span>
            <span></span>
            <span></span>
          </span>
                </div>
                <div id="navbarMenuHeroA" class="navbar-menu">

                    <div class="navbar-end">
                        <a href="#wo-für" class="navbar-item">
                            Wofür
                        </a>
                        <a href="#use-cases" class="navbar-item">
                            Use Cases
                        </a>
                        <div class="navbar-item has-dropdown is-hoverable">
                            <a href="#warum-data-versioning" class="navbar-link is-arrowless">
                                Was ist
                            </a>
                            <div class="navbar-dropdown" id="navbarMenuArchitectureDropdown">
                                <a href="#warum-data-versioning" class="navbar-item">Data Versioning</a>
                                <hr class="navbar-divider">
                                <a href="#datenquellen" class="navbar-item">Datenquellen</a>
                                <a href="#beziehungen-zwischen-entitäten" class="navbar-item">Beziehungen zwischen Entitäten</a>
                                <a href="#verteilte-datenhaltung" class="navbar-item">Verteilte Datenhaltung</a>
                                <a href="#abbruch-der-verarbeitung" class="navbar-item">Abbruch der Verarbeitung</a>
                                <a href="#big-data" class="navbar-item">Big Data</a>
                                <a href="#schema-änderungen" class="navbar-item">Schema Änderungen</a>
                                <a href="#dsgvo-konforme-datenhaltung" class="navbar-item">DSGVO konforme Datenhaltung</a>
                                <a href="#leistungen-und-garantien-der-storagekomponenten" class="navbar-item">Leistungen und Garantien der Storagekomponenten</a>
                                <a href="#parallelität-von-änderungen" class="navbar-item">Parallelität von Änderungen</a>
                                <a href="#streaming-batch-hybrid-processing-pipelines" class="navbar-item">Streaming/Batch/Hybrid Processing-Pipelines</a>
                            </div>
                        </div>
                        <div class="navbar-item has-dropdown is-hoverable">
                            <a href="#tools" class="navbar-link is-arrowless">
                                Tools
                            </a>
                            <div class="navbar-dropdown" id="navbarMenuToolsDropdown">
                                <a href="tools/lakefs.html" class="navbar-item">
                                    LakeFS
                                </a>
                                <a href="tools/dvc.html" class="navbar-item">
                                    DVC
                                </a>
                                <a href="tools/tbd.html" class="navbar-item">
                                    More To Come
                                </a>
                            </div>
                        </div>
                        <div class="navbar-item has-dropdown is-hoverable">
                            <a href="#evaluation" class="navbar-link is-arrowless">
                                Auswertungen
                            </a>
                            <div class="navbar-dropdown" id="navbarMenuEvaluationDropdown">
                                <a href="tools/tool-kontext.html" class="navbar-item">
                                    Kontext der Tools
                                </a>
                                <a href="tools/evaluation-matrix.html" class="navbar-item">
                                    Evaluations-Matrix
                                </a>
                                <a href="tools/tbd.html" class="navbar-item">
                                    More To Come
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </nav>
    </div>

  <div class="hero-body">
        <div class="">
            <p class="title" style="color: white; font-family: 'Mark Pro',sans-serif; font-weight: 800; font-size: 10vw; letter-spacing: -1px; text-transform: uppercase; line-height: 1.0; margin-bottom: 2rem;">
                Data <br><span style="color: #FF9B66;">Versioning</span>
            </p>
            <p class="subtitle" style="color: white; font-family: 'Freight Text','Georgia,Times','Times New Roman',serif; font-style: italic; font-size: 4vw; font-weight: 500;">Data Versioning From an Historical Perspective</p>
        </div>
  </div>

  <div class="hero-foot">
        <div class="columns is-mobile is-centered" style="font-size: 3em;">
            <div class="column is-half" style="text-align: center;">
                <a href="#wo-für" style="color: #FF9B66;">
                <span class="icon is-medium">
                    <i class="fa-solid fa-angle-down"></i>
                </span>
                </a>
            </div>
        </div>
    </div>
</section>



<div class="container">

<section class="section">

        <nav class="breadcrumb" aria-label="breadcrumbs">
            <ul>
                <li><a href="/">Data Mesh Architecture</a></li>
                <li><a href="/#tech-stacks">Tech Stacks</a></li>
                <li class="is-active"><a href="#" aria-current="page">Databricks</a></li>
            </ul>
        </nav>

        <h1 class="title">Databricks</h1>

        <div class="notification is-info is-light">
            Data mesh is primarily an organizational approach, and that's why you can't buy a data mesh from a vendor. Technology, however, is important still as it acts as an enabler for data mesh, and only useful and easy to use solutions will lead to domain teams' acceptance. The available offerings of cloud providers already provide a sufficient set of good self-serve data services to let you form a data platform for your data mesh. We want to show which services can be used to get started.
        </div>

        <div class="content">
            <p>
                The Databricks Lakehouse Platform is a popular data platform.
                It is fully based on <em>Apache Spark</em>.
                In fact, the people who created Apache Spark found the company to offer a commercial data platform.
                In addition, they heavily promote <em>Delta Lake</em>, a storage format for files that uses versioned Parquet files and a transaction log to provide ACID transactions.
            </p>
            <p>
                Databricks is a full-featured platform that runs on top of one of the three cloud providers: AWS, GCP, Azure.
                On Azure, Microsoft is your business partner, so you don't have to sign a separate contract with Databricks.
                In any case, the cloud provider resources (such as object storage, compute engines, virtual network configuration, access control) are used by the platform.
                The platform basically runs in compute engines and uses futher compute engines to manage workloads.
            </p>
            <p>
                Databricks is very popular among Data Scientists.
                It has an integrated environment, collaborative notebooks, and managed and scalable resources.
                But is it also a good choice for a general Data Mesh platform?
                Let's dive into the components that we would use for Data Mesh.
            </p>
            <img src="../images/databricks.png.webp" alt="Data Mesh Architecture with Databricks" style="width: 100%">

            <div class="columns">
                <div class="column">
                    <p>
                        The starting point in Databricks is the <strong>workspace</strong>.
                        Each team typically creates their own workspace,
                        and in their workspace their own <strong>clusters</strong> for their computing needs.
                        These clusters come with a configured <em>Spark Runtime</em> as query engine.
                        The clusters are mapped to cloud provider's compute instances and
                        can be scaled elastically with the workloads to execute code and queries.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/databricks_cluster.png" class="glightbox">
                            <img src="../images/databricks_cluster.png" alt="Databricks requires a Spark cluster to run code.">
                        </a>
                    </figure>
                </div>
            </div>

            <div class="columns">
                <div class="column">
                    <p>
                        Data are always stored as files.
                        To ingest data, data need to be written to an object store (S3, Cloud Storage, ADLS) by a provider in an appropriate format, such as Parquet, JSON, CSV or Delta.
                        Transformed and intermediary data are usually written to <strong>delta files</strong> in a backing object storage, but Spark supports all other formats, if required.
                        Delta files represent a table structure, and are optimized for columnar access.
                        These delta tables also support insert, update and delete operations.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/databricks_files.png" class="glightbox">
                            <img src="../images/databricks_files.png" alt="Databricks relies on files on the object store">
                        </a>
                    </figure>
                </div>
            </div>

            <div class="columns">
                <div class="column">
                    <p>
                        With Spark, most of the data engineering is typically coded in Jupyter-like <strong>notebooks</strong> using <em>Python</em>, <em>Scala</em>, or <em>Spark SQL</em> code.
                        The notebooks are interactive, meaning that cells can be executed directly and iteratively.
                        With Databricks, data import, cleaning, transformation, explorative analytics, and machine learning is coded in notebooks.
                        The code can usually be managed in a connected Git repository.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/databricks_notebook.png" class="glightbox">
                            <img src="../images/databricks_notebook.png" alt="A Notebook in Databricks contains code">
                        </a>
                    </figure>
                </div>
            </div>

            <div class="columns">
                <div class="column">
                    <p>
                        Notebooks are also the default way to build <strong>data products</strong>.
                        <em>Delta live tables</em> are used to build managed pipelines with notebooks, for scheduled or continuous tranformations.
                        Notebooks include the code that imports data, transforms and aggregates, performs validation, includes tests and quality metrics and finally creates a data set in the export format.
                        Through Spark, there is a lot of freedom how to operate with the data and available libraries (especially for Python) provide unlimited capabilities.
                        For example, a data product notebook may also include a step to generate and publish a data product description for documentation, including dynamic information on the current data set.
                        Also, <strong>policy automation</strong> can be implemented this way, e.g. by including a shared library that performs anonymization procedures on tagged columns.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/databricks_dlt.png" class="glightbox">
                            <img src="../images/databricks_dlt.png" alt="Delta live tables can be used as managed pipelines">
                        </a>
                    </figure>
                </div>
            </div>

            <div class="columns">
                <div class="column">
                    <p>
                        A <strong>metastore</strong> maps table structures to underlying files in the object store.
                        This allows you to execute structured SQL queries to files and to evolve the schema over time.
                        Traditionally, most Data Lakes rely on Apache Hive, a metastore coming from Hadoop.
                        As Hive has some drawbacks (e.g., difficult to install and maintain, requires a relational database).
                        Databricks recently implemented an alternative to Hive, the <em>Unity Catalog</em>, a proprietary metastore for Databricks.
                        A Unity Catalog can be shared across workspaces, as it is backed by a simple object store.
                    </p>
                    <p>
                        All tables and columns are registered in the metastore.
                        This makes the metastore a candidate for a <strong>data catalog</strong> to publish, manage and discover data products.
                        A frequent challenge with a metastore as a data catalog is that it quickly becomes clattered and convoluted with internal and irrelevant data structures, important data products may perish.
                        Unity Catalog is quite new, but certainly will become the default data catalog solution for a Databricks-only data platform concept.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/databricks_catalog.png" class="glightbox">
                            <img src="../images/databricks_catalog.png" alt="">
                        </a>
                    </figure>
                </div>
            </div>

            <div class="columns">
                <div class="column">
                    <p>
                        Databricks Premium comes with special view for data analysts that prefer to work with SQL.
                        Historically, it was just an SQL endpoint to access data on Databricks, but additional services were added, providing a more data-warehouse-like experience.
                        SQL features can be used for data mesh capabilities:
                        Software engineers can use SQL for simple analytics, without the need to work with notebooks, and they can leverage <em>SQL dashboards</em> and visualizations to gain <strong>insights</strong>.
                        <em>SQL alerts</em> can be used for basic <strong>data quality monitoring</strong>, as they run queries regularly and send notifications to a when the queries trigger.
                        The SQL warehouse provides an HTTP or JDBC <em>connection endpoint</em> to publish data products for external applications, such as Tableau or PowerBI.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/databricks_sql.png" class="glightbox">
                            <img src="../images/databricks_sql.png" alt="SQl queries with Databricks">
                        </a>
                    </figure>
                </div>
            </div>

            <p>
                Databricks is probably the most powerful data platform available today, and it clearly can act as a foundation for a data mesh platform.
                It is optimized for data scientists that work a lot with notebooks, but it also
                invested in further experiences.
                As it tightly interacts with the cloud provider for data storage and access management, organization and administration will take a while to master for the data platform team, and it must be set up appropriately from the beginning.
                Databricks is a great platform for data analysts and machine learning, but the entry barrier is pretty high for occasional users.
                Domain team's software engineers who use data analytics from time to time and build just a few data products,
                may need a lot of guidance by an enabling team to work efficiently and to gain a positive developer experience.<br>
            </p>

            <h5>References</h5>
            <ul>
                <li><a href="https://docs.databricks.com/">Official databricks documentation</a></li>
                <li><a href="https://www.youtube.com/watch?v=a00rdlNtW98">Data Mesh and Data Lakehouse talk by co-founder of Databricks</a></li>
            </ul>

        </div>
    </section>

</div>



<footer class="footer">
    <div class="content has-text-centered">
        <p>
            <a href="https://www.innoq.com">
                <img src="images/supported-by-innoq--petrol-apricot.svg" alt="Supported by INNOQ" class="footer-logo" width="180" />
            </a>
        </p>
        <p>
            <a href="https://www.innoq.com/en/topics/data-mesh-workshop?ref=dma-footer">Workshop</a>&nbsp
            <a href="https://www.socreatory.com/de/trainings/datamesh?ref=dma-footer">Training</a>&nbsp
            <a href="https://www.innoq.com/en/impressum/">Legal Notice</a>&nbsp
            <a href="https://www.innoq.com/en/datenschutz/">Privacy</a>
        </p>
    </div>
</footer>

<script src="js/navigation.js"></script>

<script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<script async src="https://scripts.simpleanalyticscdn.com/auto-events.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript>

<link rel="stylesheet" href="css/glightbox.css" />
<script src="js/glightbox.js"></script>
<script type="text/javascript">
    const lightbox = GLightbox({});
</script>


<script src="js/anchor.min.js"></script>
<script>anchors.add();</script>
</body>

</html>
